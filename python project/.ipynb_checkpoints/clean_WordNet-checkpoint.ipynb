{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "### This notebook precomputes all the query expansion terms from the WordNet API\n",
    "\n",
    "Uses word sense disambiguation to determine an accurate sense for each ambiguous word in a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "from nltk.stem import PorterStemmer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "stop_words = {\"is\", \"a\", \"about\", \"above\", \"all\", \"along\",\"also\", \"although\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren't\", \"as\", \"at\",\"be\", \"because\", \"been\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"couldn't\",\"did\", \"didn't\", \"do\", \"does\", \"doesn't\", \"e.g.\", \"either\", \"etc\", \"etc.\",\"even\", \"ever\", \"enough\", \"for\", \"from\", \"further\", \"get\", \"gets\", \"got\", \"had\", \"have\",\"hardly\", \"has\", \"hasn't\", \"having\", \"he\", \"hence\", \"her\", \"here\",\"hereby\", \"herein\", \"hereof\", \"hereon\", \"hereto\", \"herewith\", \"him\",\"his\", \"how\", \"however\", \"i\", \"i.e.\", \"if\", \"in\", \"into\", \"it\", \"it's\", \"its\",\"me\", \"more\", \"most\", \"mr\", \"my\", \"near\", \"nor\", \"now\", \"no\", \"not\", \"or\", \"on\", \"of\", \"onto\",\"other\", \"our\", \"out\", \"over\", \"really\", \"said\", \"same\", \"she\",\"should\", \"shouldn't\", \"since\", \"so\", \"some\", \"such\",\"than\", \"that\", \"the\", \"their\", \"them\", \"then\", \"there\", \"thereby\",\"therefore\", \"therefrom\", \"therein\", \"thereof\", \"thereon\", \"thereto\",\"therewith\", \"these\", \"they\", \"this\", \"those\", \"through\", \"thus\", \"to\",\"too\", \"under\", \"until\", \"unto\", \"upon\", \"us\", \"very\", \"was\", \"wasn't\",\"we\", \"were\", \"what\", \"when\", \"where\", \"whereby\", \"wherein\", \"whether\",\"which\", \"while\", \"who\", \"whom\", \"whose\", \"why\", \"with\", \"without\",\"would\", \"you\", \"your\", \"yours\", \"yes\"}\n",
    "def trim(words, min_word_len=2):\n",
    "    \"\"\"Removes stop words and small words from a string\"\"\"\n",
    "    words = set(words)\n",
    "    words -= stop_words\n",
    "    words = {x.lower() for x in words if len(x) >= min_word_len}\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation\n",
    "http://www.nltk.org/howto/wsd.html\n",
    "\n",
    "Citation: https://dl.acm.org/citation.cfm?id=318728\n",
    "\n",
    "Performs the classic Lesk algorithm for Word Sense Disambiguation (WSD) using a the definitions of the ambiguous word.\n",
    "\n",
    "Given an ambiguous word and the context in which the word occurs, Lesk returns a Synset with the highest number of overlapping words between the different definitions from each Synsets of each word in the context sentence and different definitions from each Synset of the ambiguous word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLemmaDict(query):\n",
    "    if type(query) is not set:\n",
    "        sent = set(query.split())\n",
    "    sent = trim(sent)\n",
    "    lDict = {}\n",
    "    for term in sent:\n",
    "        syn = lesk(sent, term)\n",
    "        s_term = ps.stem(term)\n",
    "        s_lemmas = None\n",
    "        if syn is not None:\n",
    "            # Remove multi term lemmas\n",
    "            lemmas = [l.name() for l in syn.lemmas() if '_' not in l.name()]\n",
    "            # Stem and remove terms that stem to the same value\n",
    "            s_lemmas = set([ps.stem(l) for l in lemmas]) - set([s_term])\n",
    "            if len(s_lemmas) == 0:\n",
    "                s_lemmas = None\n",
    "        lDict[s_term] = s_lemmas\n",
    "    return lDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aerodynam': {'flow', 'sleek', 'streamlin'},\n",
       " 'applic': None,\n",
       " 'chemic': None,\n",
       " 'hyperson': None,\n",
       " 'kinet': None,\n",
       " 'problem': {'troubl'},\n",
       " 'system': None}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getLemmaDict('what chemical kinetic system is applicable to hypersonic aerodynamic problems')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute query substitution terms for each query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'', 'aeroelastic', 'models', 'high', 'be', 'obeyed', 'heated', 'what', 'constructing', 'speed', 'must', 'similarity', 'aircraft', 'laws', 'of', 'when'} 4\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'sent' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-1928d945e627>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlineNum\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineNum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                     \u001b[0mqSubs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetLemmaDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mterms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-117-017410ab2269>\u001b[0m in \u001b[0;36mgetLemmaDict\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mlDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'sent' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Gather unique query terms\n",
    "import re\n",
    "qSubs = {}\n",
    "for path, fkey in [('../datasets/cran/cran.qry', 'cran'), ('../datasets/adi/ADI.QRY', 'adi'), \n",
    "             ('../datasets/med/MED.QRY', 'med'), ('../datasets/time/TIME_clean.QUE', 'time')]:\n",
    "    qSubs[fkey] = {}\n",
    "    with open(path, 'r') as qfile: \n",
    "        lineNum = 0\n",
    "        for line in qfile:\n",
    "            # Finished processing last query. \n",
    "            # Compute substitution terms. \n",
    "            # Get ID for next query.\n",
    "            if '.I' in line:\n",
    "                if lineNum != 0:\n",
    "                    print(terms, lineNum)\n",
    "                    qSubs[fkey][ID] = getLemmaDict(terms)\n",
    "                    break\n",
    "                terms = set()\n",
    "                ID = line.split()[1]\n",
    "            elif '.W' not in line:\n",
    "                terms |= set(re.split('[^a-zA-Z]+', line))\n",
    "            lineNum += 1\n",
    "        break\n",
    "    break\n",
    "qSubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
