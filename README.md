# Information Retrieval Research on Term Proximity
#### Author: Eric LaBouve

### Background:
Information retrieval is the field of computer science that focuses on recovering information from stored data. In this research project, we are specifically focusing on retrieving relevant documents according to user defined queries. Previous standards for document recovery systems focus on a bag of words model -which disregards grammar and word ordering. Bag of words models can rank documents based on vector space mathematical model where the terms discovered in the documents give a coordinate in hyperspace. We can then compute the angle between these hyperspace vectors to determine their similarity. Another information retrieval approach that utilizes a bag of words model is the Okapi BM25 distance formula. Okapi BM25 performs better than the vector space model for short query retrievals and takes into account term frequencies, term relevance, document length, and the query.

### Research: 
A down side to a bag of words approach is the model's disregard for term positioning inside documents. We would like to expand the bag of words model by taking into account query terms and their proximities inside documents. For this, we pay close attention to how terms are ordered in the query. It is reasonable to assume that if there exist two documents that both contain a similar number of query terms, then the document that contains the query terms in a closer proximity and ordering as they appear in the query would be more relevant to the query than the other document. From this observation, we can conclude that each document has a query specific structure that can be expressed through both the query terms and query term locations. This structure for a document can be represented as a graph whose nodes are the query terms and whose edges are the distances between the query terms. Another observation that can be made is the ordering of words in the query. Because text in English is read from left to right, we enforce another constraint on our document graph. Edges may only connect terms, T1 and T2, if the index of T2 is greater than the index of T1 in both the document and query. In this example, T1 is represented as T2's parent node. The resulting graph is not necessarily connected and may have multiple root nodes. 

After a graph for the document is generated, there needs to be a way to extract information from the graph and eventually obtain a score that relates the query to the document. One way to extract information from the graph is to search for paths that include the most query terms in the smallest area of the document. I call this approach the 2-Constraint problem because we are trying to simultaneously maximize the number of nodes in our path while minimizing the distances between these nodes. Upon further consideration of the 2-Constraint problem, we can define a function that takes into account the number of nodes in a path and the total length of the path in order to assign a numeric score for this path. This score can be used to rank our document against our query. 

One way that the method above can fail is if relevant documents contain synonyms of our query terms. So far, our system only takes into account the exact terms that appear in the query. In order to obtain more accurate results, we can perform a query expansion. Using the graph generated using WordNet, we can replace query terms with terms that differ slightly in definition but may intersect more with the terms that appear in relevant documents. The exact implementation details are not yet complete, but we believe that an appropriate method for this is as follows. For each query, we generate 20 new queries using similar words to the query terms and then make a unique graph for each of these queries on the document. We then score the query using a function that takes into account the number of nodes in a path, the total length of the path, and a sense score for each of the terms used in order to produce a score for the document.

An additional way to improve the system will draw from the vector space model. We can edit our scoring function to take into account term frequencies and inverse document frequencies in order to weight the importance of terms to each other. For example, terms that appear in all documents are less relevant than terms that hardly ever appear. So if a query includes a rare term, documents that also include this rare term should be ranked as more relevant.

Lastly, a single optimum path through the generated graph might not be efficient enough to accurately represent the document. Experiments can be done to take into account the top X paths discovered in the document. Although this may give preference to longer documents as longer documents are more likely to contain more paths, research can be conducted to normalize this issue.
